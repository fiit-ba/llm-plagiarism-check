{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dspy\n",
    "from dspy.teleprompt import BootstrapFewShot\n",
    "from dspy.evaluate import Evaluate\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from dataloader import build_eval_dataset, check_if_data_folder_exits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "AZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "AZURE_OPENAI_VERSION = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    "\n",
    "DATA_FOLDER = \"data/IR-Plag-Dataset\"\n",
    "check_if_data_folder_exits(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval dataset for solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = build_eval_dataset(DATA_FOLDER)\n",
    "eval_df.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.tsv\", sep=\"\\t\")\n",
    "df = df.sample(frac=1, random_state=1337).reset_index(drop=True)\n",
    "\n",
    "def create_example(row: pd.Series) -> dspy.Example:\n",
    "    return dspy.Example(\n",
    "        code_sample_1=row[\"sample_1\"],\n",
    "        code_sample_2=row[\"sample_2\"],\n",
    "        plagiarized=\"Yes\" if row[\"plagiarized\"] else \"No\",\n",
    "        explanation=row[\"reason\"],\n",
    "    ).with_inputs(\"code_sample_1\", \"code_sample_2\")\n",
    "\n",
    "\n",
    "train_examples = []\n",
    "for _, row in df.iterrows():\n",
    "    example = create_example(row)\n",
    "    train_examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy\n",
    "\n",
    "our \"8 steps of using DSPy\" could be find in [README.md](README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm setup\n",
    "lm = dspy.AzureOpenAI(\n",
    "    api_base=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=AZURE_OPENAI_VERSION,\n",
    "    deployment_id=AZURE_OPENAI_DEPLOYMENT,\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    ")\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Signature(dspy.Signature):\n",
    "    \"\"\"Detect if two code samples are plagiarized. In plagiarized field answer only : Yes if the code samples are plagiarized, No otherwise. In explenation field add the reason why the code samples are/ are not plagiarized.\"\"\"\n",
    "\n",
    "    code_sample_1 = dspy.InputField(desc=\"The first code sample to compare\")\n",
    "    code_sample_2 = dspy.InputField(desc=\"The second code sample to compare\")\n",
    "    explanation = dspy.OutputField(\n",
    "        desc=\"Explanation or reason why the code samples are/ are not plagiarized\"\n",
    "    )\n",
    "    plagiarized = dspy.OutputField(\n",
    "        desc=\"Yes/No indicating if code samples are plagiarized\"\n",
    "    )\n",
    "\n",
    "\n",
    "class CoT(dspy.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(Signature)\n",
    "\n",
    "    def forward(self, code_sample_1: str, code_sample_2: str) -> Signature:\n",
    "        return self.prog(code_sample_1=code_sample_1, code_sample_2=code_sample_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_answer(\n",
    "    example: dspy.Example, pred: Signature, trace: object = None\n",
    ") -> bool:\n",
    "    score = True if pred.plagiarized.lower() == example.plagiarized.lower() else False\n",
    "    # print(f\"Pred: {pred.plagiarized} | Actual: {example.plagiarized} | Match: {score}\")\n",
    "    return score\n",
    "\n",
    "evaluate = Evaluate(\n",
    "    devset=train_examples,\n",
    "    metric=validate_answer,\n",
    "    num_threads=4,\n",
    "    display_progress=True,\n",
    "    display_table=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot evaluation on train data\n",
    "evaluate(CoT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"max_bootstrapped_demos\": 8, \"max_labeled_demos\": 8}\n",
    "\n",
    "teleprompter = BootstrapFewShot(metric=validate_answer, **config)\n",
    "optimized_cot = teleprompter.compile(CoT(), trainset=train_examples)\n",
    "evaluate(optimized_cot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
