{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import dspy\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import (\n",
    "    BootstrapFewShot,\n",
    "    BootstrapFewShotWithRandomSearch,\n",
    "    KNNFewShot,\n",
    "    MIPROv2,\n",
    ")\n",
    "\n",
    "from dataloader import build_eval_dataset, check_if_data_folder_exits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# azure-openai-gpt-4o\n",
    "AZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "AZURE_OPENAI_VERSION = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    "\n",
    "# azure-openai-gpt-35-turbo\n",
    "AZURE_OPENAI_KEY_35_TURBO = os.getenv(\"AZURE_OPENAI_KEY_35_TURBO\")\n",
    "AZURE_OPENAI_VERSION_35_TURBO = os.getenv(\"AZURE_OPENAI_VERSION_35_TURBO\")\n",
    "AZURE_OPENAI_DEPLOYMENT_35_TURBO = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_35_TURBO\")\n",
    "AZURE_OPENAI_ENDPOINT_35_TURBO = os.getenv(\"AZURE_OPENAI_ENDPOINT_35_TURBO\")\n",
    "\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\")\n",
    "\n",
    "DATA_FOLDER = \"data/IR-Plag-Dataset\"\n",
    "check_if_data_folder_exits(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval dataset for solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = build_eval_dataset(DATA_FOLDER)\n",
    "eval_df.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.tsv\", sep=\"\\t\")\n",
    "df = df.sample(frac=1, random_state=1337).reset_index(drop=True)\n",
    "\n",
    "def create_example(row: pd.Series) -> dspy.Example:\n",
    "    return dspy.Example(\n",
    "        code_sample_1=row[\"sample_1\"],\n",
    "        code_sample_2=row[\"sample_2\"],\n",
    "        plagiarized=\"Yes\" if row[\"plagiarized\"] else \"No\",\n",
    "        explanation=row[\"reason\"],\n",
    "    ).with_inputs(\"code_sample_1\", \"code_sample_2\")\n",
    "\n",
    "\n",
    "train_examples = []\n",
    "for _, row in df.iterrows():\n",
    "    example = create_example(row)\n",
    "    train_examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy\n",
    "\n",
    "our \"8 steps of using DSPy\" could be find in [README.md](README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm setup\n",
    "llm_name = \"azure-gpt-4o\"\n",
    "# llm_name = \"ollama-codellama:13b\"\n",
    "# llm_name = \"azure-gpt-35-turbo\"\n",
    "program_save_path = \"programs/{llm_name}_{optimizer}_{score}\"\n",
    "\n",
    "\n",
    "if llm_name == \"azure-gpt-4o\":\n",
    "    lm = dspy.AzureOpenAI(\n",
    "        api_base=AZURE_OPENAI_ENDPOINT,\n",
    "        api_version=AZURE_OPENAI_VERSION,\n",
    "        deployment_id=AZURE_OPENAI_DEPLOYMENT,\n",
    "        api_key=AZURE_OPENAI_KEY,\n",
    "    )\n",
    "elif llm_name == \"azure-gpt-35-turbo\":\n",
    "    lm = dspy.AzureOpenAI(\n",
    "        api_base=AZURE_OPENAI_ENDPOINT_35_TURBO,\n",
    "        api_version=AZURE_OPENAI_VERSION_35_TURBO,\n",
    "        deployment_id=AZURE_OPENAI_DEPLOYMENT_35_TURBO,\n",
    "        api_key=AZURE_OPENAI_KEY_35_TURBO,\n",
    "    )\n",
    "elif \"ollama\" in llm_name:\n",
    "    model_name = \"-\".join(llm_name.split(\"-\")[1:])\n",
    "    lm = dspy.OllamaLocal(base_url=OLLAMA_URL, model=model_name)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown LLM name: {llm_name}\")\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "metadata = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Signature(dspy.Signature):\n",
    "    \"\"\"Detect if two code samples are plagiarized. In plagiarized field answer only : Yes if the code samples are plagiarized, No otherwise. In explenation field add the reason why the code samples are/ are not plagiarized.\"\"\"\n",
    "\n",
    "    code_sample_1 = dspy.InputField(desc=\"The first code sample to compare\")\n",
    "    code_sample_2 = dspy.InputField(desc=\"The second code sample to compare\")\n",
    "    explanation = dspy.OutputField(\n",
    "        desc=\"Explanation or reason why the code samples are/ are not plagiarized\"\n",
    "    )\n",
    "    plagiarized = dspy.OutputField(\n",
    "        desc=\"Yes/No indicating if code samples are plagiarized\"\n",
    "    )\n",
    "\n",
    "\n",
    "class CoT(dspy.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(Signature)\n",
    "\n",
    "    def forward(self, code_sample_1: str, code_sample_2: str) -> Signature:\n",
    "        return self.prog(code_sample_1=code_sample_1, code_sample_2=code_sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_answer(\n",
    "    example: dspy.Example, pred: Signature, trace: object = None\n",
    ") -> bool:\n",
    "    try:\n",
    "        if pred.plagiarized is None:\n",
    "            return False  # or handle this case as appropriate for your use case\n",
    "\n",
    "        pred_plag = pred.plagiarized.strip().lower().split(\"\\n\")[0]\n",
    "        yes_no_pattern = r\"\\b(yes|no)\\b\"\n",
    "        match = re.search(yes_no_pattern, pred_plag)\n",
    "        extracted_answer = match.group(1) if match else pred.plagiarized.strip().lower()\n",
    "\n",
    "        if example.plagiarized is None:\n",
    "            return False  # or handle this case as appropriate for your use case\n",
    "\n",
    "        score = (\n",
    "            True if extracted_answer == example.plagiarized.strip().lower() else False\n",
    "        )\n",
    "    except Exception:\n",
    "        score = False\n",
    "    return score\n",
    "\n",
    "\n",
    "evaluate = Evaluate(\n",
    "    devset=train_examples,\n",
    "    metric=validate_answer,\n",
    "    num_threads=4,\n",
    "    display_progress=True,\n",
    "    display_table=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot evaluation on train data\n",
    "score = evaluate(CoT())\n",
    "\n",
    "metadata.append({\"optimizer\": \"zero-shot\", \"score\": score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bootstrapfewshot(\n",
    "    program: dspy.Module, llm_name: str, train_examples: list, metric: callable\n",
    ") -> object:\n",
    "    optimizer = \"BootstrapFewShot\"\n",
    "    config = {\"max_bootstrapped_demos\": 8, \"max_labeled_demos\": 8}\n",
    "\n",
    "    teleprompter = BootstrapFewShot(metric=metric, **config)\n",
    "    optimized_cot = teleprompter.compile(program, trainset=train_examples)\n",
    "    score = evaluate(optimized_cot)\n",
    "\n",
    "    save_path = program_save_path.format(\n",
    "        llm_name=llm_name, optimizer=optimizer, score=round(score, 2)\n",
    "    )\n",
    "    metadata = {\"optimizer\": optimizer, \"score\": score, \"save_path\": save_path}\n",
    "    optimized_cot.save(save_path)\n",
    "    return metadata\n",
    "\n",
    "\n",
    "metadata.append(run_bootstrapfewshot(CoT(), llm_name, train_examples, validate_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bootstrapfewshotwithrandomsearch(\n",
    "    program: dspy.Module, llm_name: str, train_examples: list, metric: callable\n",
    ") -> object:\n",
    "    optimizer = \"BootstrapFewShotWithRandomSearch\"\n",
    "    config = {\n",
    "        \"max_bootstrapped_demos\": 8,\n",
    "        \"max_labeled_demos\": 8,\n",
    "        \"num_candidate_programs\": 20,\n",
    "        \"num_threads\": 4,\n",
    "    }\n",
    "\n",
    "    teleprompter = BootstrapFewShotWithRandomSearch(metric=metric, **config)\n",
    "    optimized_cot = teleprompter.compile(program, trainset=train_examples)\n",
    "    score = evaluate(optimized_cot)\n",
    "    save_path = program_save_path.format(\n",
    "        llm_name=llm_name, optimizer=optimizer, score=round(score, 2)\n",
    "    )\n",
    "    metadata = {\"optimizer\": optimizer, \"score\": score, \"save_path\": save_path}\n",
    "    optimized_cot.save(save_path)\n",
    "    return metadata\n",
    "\n",
    "\n",
    "metadata.append(\n",
    "    run_bootstrapfewshotwithrandomsearch(\n",
    "        CoT(), llm_name, train_examples, validate_answer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knnfewshot(\n",
    "    program: dspy.Module, llm_name: str, train_examples: list, metric: callable\n",
    ") -> object:\n",
    "    optimizer = \"KNNFewShot\"\n",
    "    knn_teleprompter = KNNFewShot(7, train_examples)\n",
    "    optimized_cot = knn_teleprompter.compile(CoT(), trainset=train_examples)\n",
    "    score = evaluate(optimized_cot)\n",
    "    save_path = program_save_path.format(\n",
    "        llm_name=llm_name, optimizer=optimizer, score=round(score, 2)\n",
    "    )\n",
    "    metadata = {\"optimizer\": optimizer, \"score\": score, \"save_path\": save_path}\n",
    "    optimized_cot.save(save_path)\n",
    "    return metadata\n",
    "\n",
    "\n",
    "metadata.append(run_knnfewshot(CoT(), llm_name, train_examples, validate_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_miprov2(\n",
    "    program: dspy.Module,\n",
    "    llm_name: str,\n",
    "    train_examples: list,\n",
    "    metric: callable,\n",
    "    prompt_model: object,\n",
    "    task_model: object,\n",
    ") -> object:\n",
    "    optimizer = \"MIPROv2\"\n",
    "    n = 20  # The number of instructions and fewshot examples that we will generate and optimize over\n",
    "    batches = 40  # The number of optimization trials to be run (we will test out a new combination of instructions and fewshot examples in each trial)\n",
    "    temperature = 0.5  # The temperature configured for generating new instructions\n",
    "    eval_kwargs = {\"num_threads\": 4, \"display_progress\": True, \"display_table\": 0}\n",
    "    teleprompter = MIPROv2(\n",
    "        prompt_model=lm,\n",
    "        task_model=lm,\n",
    "        metric=validate_answer,\n",
    "        num_candidates=n,\n",
    "        init_temperature=temperature,\n",
    "        verbose=True,\n",
    "    )\n",
    "    optimized_cot = teleprompter.compile(\n",
    "        CoT(),\n",
    "        trainset=train_examples,\n",
    "        num_batches=batches,\n",
    "        max_bootstrapped_demos=16,\n",
    "        max_labeled_demos=16,\n",
    "        requires_permission_to_run=False,\n",
    "        eval_kwargs=eval_kwargs,\n",
    "    )\n",
    "    score = evaluate(optimized_cot)\n",
    "    save_path = program_save_path.format(\n",
    "        llm_name=llm_name, optimizer=optimizer, score=round(score, 2)\n",
    "    )\n",
    "    metadata = {\"optimizer\": optimizer, \"score\": score, \"save_path\": save_path}\n",
    "    optimized_cot.save(save_path)\n",
    "    return metadata\n",
    "\n",
    "\n",
    "metadata.append(run_miprov2(CoT(), llm_name, train_examples, validate_answer, lm, lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(metadata)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"data/metadata/{llm_name}_metadata.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
